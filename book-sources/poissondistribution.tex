\documentclass[stochastic-or.tex]{subfiles}
\input{header}
\begin{document}

\section{Poisson Distribution}\label{sec:poisson-distribution}

In queueing systems the goal is to meet supply with demand at reasonable cost.
For this to work, we need to characterize demand, and one of the most reasonable models for the arrival process of jobs\sidenote{A job can be anything that requires service; it Ã¦can be a customer, an item to make, a car to repair,\ldots} is the Poisson process.
We derive a number of its properties in the text and in the exercises; we use these properties throughout the book.



\newthought{Consider a stream} of customers that enter a shop in a sequence of time intervals, each with a duration of~$t$ minutes, and such that the expected number of arrivals in  any such interval is $\lambda t$.
Let us suppose we have $\lambda t n$ customers that select, uniformly distributed, the first interval out of these~$n$ intervals to enter the shop.
Let the total number of arrivals be given by the rv $N_n(t)$. It is well-known that $N_{t} \sim \Bin{\lambda t n, p}$, i..e.,
\begin{equation*}
 \P{N_n(t) = k} = {\lambda t n \choose k} p^k (1-p)^{\lambda t n-k},
\end{equation*}
where  $p=1/n$ is the success probability, that is, the probability that some customer chooses the first interval.
Clearly, the expected number of arrivals during this first interval $[0,t]$ is $\E{N_n(t)} = \lambda t n p = \lambda t$, because $p=1/n$.

If we take the limit $n\to\infty$, then since $p=1/n$,  the expected number of arrivals during the first interval $[0,t]$ remains equal to $\lambda t$,
and,\sidenote{\cref{ex:31}}
\begin{equation}\label{eq:bin}
  \lim_{\stackrel{n\to\infty, p\to 0}{np=1}}{\lambda t n \choose k} p^k (1-p)^{\lambda t n -k} = e^{-\lambda t}\frac{(\lambda t)^k}{k!}.
\end{equation}
We say that the random variable $N(t)$
associated with this distribution is \recall{Poisson distributed} with parameter $\lambda t$, i.e.,
\begin{equation*}
 \P{N(t) = k} =
e^{-\lambda t} \frac{(\lambda t)^k}{k!},
\end{equation*}
and we write $N(t)\sim \Pois{\lambda t}$.%\sidenote{\cref{ex:p2}}
From this it is immediate that
\begin{align}
\E{N(t)} &= \lambda t
%\E{(N(t))^2} &= \lambda^2 t^2 + \lambda t,\\
& \V{N(t)} &= \lambda t.
\end{align}
%\end{subequations}



\newthought{The family} of random variables $N_\lambda=\{N(t), t\geq 0\}$ is a \recall{Poisson process} with \emph{arrival rate} $\lambda$.\sidenote{ A random process is a much more complicated mathematical object than a random variable: a process is a (possibly uncountable) set of random variables indexed by time, not just \emph{one} random variable.}
Once we have $N(t)$, we can define $N(s, t] := N(t)-N(s)$ as the number of customers that arrive in a general time period $(s, t]$.\sidenote{Note that $[0,t]$ is closed at both ends, but $(s,t]$ is open at the left.}

It is important to realize that  $N_\lambda$ has \recall{stationary and independent increments}.
Stationarity means that the distributions of the number of arrivals are the same for all intervals of equal length, that is,
$N(s,t]$ has the same probability distribution as $N(u, v]$ if $t-s = v-u$.
Independence means, roughly speaking, that knowing that $N(s,t]= n$, does not help to make any predictions about the value of $N(u, v]$ if the intervals $(s,t]$ and $(u, v]$ do not overlap.\sidenote{We refer to the literature on (mathematical) probability theory for further background.}

We next address a number of useful properties of the Poisson process.
If $\Delta t\ll 1$ then for all $t\geq 0$,
\begin{subequations}\label{eq:32}
\begin{align}
\P{N(t+\Delta t) = n \given N(t) = n} &= 1-\lambda \Delta t + o(\Delta t),  \\
\P{N(t+\Delta t) = n+1 \given N(t) = n} &= \lambda \Delta t + o(\Delta t), \label{eq:8} \\
\P{N(t+\Delta t) \geq n+2 \given N(t) = n} &= o(\Delta t).\label{eq:10}
\end{align}
\end{subequations}
To see how to derive the first of these estimates,  we use the definition of the conditional probability and small~$o$ notation, and get
% \begin{align*}
%  \1{N(t+\Delta t)=n, N(t)=n}
% &= \1{N(t) + N(t, t+\Delta t] = n, N(t)=n} = \1{N(t, t+\Delta t] = 0, N(t)=n}.
% \end{align*}
% Thus,
 \begin{align*}
 \P{N(t+\Delta t) = n \given N(t) = n}
&= \frac{\P{N(t+\Delta t) = n, N(t) = n}}{\P{N(t)=n}} \\
&\stackrel1= \frac{\P{N(t, t+\Delta t] = 0, N(t) = n}}{\P{N(t)=n}} \\
&\stackrel2= \frac{\P{N(t, t+\Delta t] = 0} \P{N(t) = n}}{\P{N(t)=n}} \\
%& \quad\text{(by independence of } N(t, t+h] \text{ and } N(t))\\
%&= \P{N(t, t+\Delta t] = 0}
&\stackrel3= \P{N(0, \Delta t] = 0} \\
&\stackrel4= e^{-\lambda \Delta t} (\lambda \Delta t)^0/0!
= e^{-\lambda \Delta t} = 1-\lambda \Delta t + o(\Delta t).
 \end{align*}
Step 1 is true because $N(t+\Delta t]= N(t) + N(t, t+\Delta t]$, step 2 follows from independence, step 3 from stationarity, step 4 from the definition of the Poisson distribution.


The next equation says that if you know that an arrival occurred during $[0,t]$, the arrival is distributed uniformly on the interval $[0,t]$.
If $s\in [0,t]$,\sidenote{\cref{ex:p-36} }
\begin{equation}\label{eq:22}
  \P{N(s) =1\given N(t)=1} = \frac s t.
\end{equation}
Note that this is independent of $\lambda$.



\newthought{Merging and splitting} of arrival processes often occurs in practice.
Consider, for instance, the arrival processes $N_\lambda$ of men and $N_\mu$ of women at a shop, see the figure at the right.
\begin{marginfigure}
\begin{tikzpicture}[xscale=0.3]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);
\end{tikzpicture}
\end{marginfigure}
Each cross represents an arrival; in the upper line it corresponds to a man, in the middle line to a woman, and in the lower line to an arrival of a general customer at the shop.
Thus, the shop `sees' the merged process of these two arrival processes.
In fact, this merged process $N_{\lambda+\mu}$ is also a Poisson process\sidenote{\cref{ex:l-103} } with rate $\lambda+\mu$.

We can also \emph{split}, or \emph{thin}, a stream into several sub-streams.
Model the stream of people passing by a shop as a Poisson process $N_\lambda$, and the decision $B_{i}\in \{0, 1\}$ that customer~$i$ enters the shop as an  Bernoulli distributed rv with success probability $\P{B_{i} = 1} = 1$.
\begin{marginfigure}
\begin{tikzpicture}[xscale=0.4]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(6,2);
\node[left] at (0,2) {$N_\lambda$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(6,0);
\node[left] at (0,0) {$N_{\lambda p}$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06)
node[below] {$B_1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3)
node[below] {$B_2$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3)
node[below, fill=white] {$B_3$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06)
node[below] {$B_4$};

% \draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06)
% node[below] {$B_5=1$};


% \draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3)
% node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
 \end{marginfigure}
 In the figure at the right, we mark the arrivals of all potential customers as crosses at the upper line.
The lines downward represent whether a potential customer actually enters the shops. Now, when $\{B_{i}\}$ forms a set of iid rvs, then the thinned process is also a Poisson process\sidenote{\cref{ex:1}} but  with rate $\lambda p$.



The concepts of \recall{merging} and \recall{thinning} are useful to analyze queueing networks, see~\cref{sec:jackson-networks}.
Suppose the departure stream of a machine splits into two sub-streams, e.g., a fraction~$p$ of the jobs moves on to another machine and the rest ($1-p$) of the jobs leaves the system.
Then we can model the arrival stream at the second machine as a thinned stream (with probability~$p$) of the departures of the first machine.
Merging occurs where the output streams of various stations arrive at another station.

% \newthought{With moment-generating functions} we can simplify the derivations above; the last few exercises of this section show how to apply this.\sidenote{In general, it is hard to obtain closed-form expressions for the moment-generating function, but when it works, it is an easy and slick technique.}


% \begin{exercise}\label{ex:p1}
% Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$.
% \begin{hint}
% Use that $\E{X+Y} = \E X + \E Y$.
% \end{hint}
% \begin{solution}
%  \begin{equation*}
%  \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
%  \end{equation*}
% \end{solution}
% \end{exercise}



% \begin{exercise}\label{ex:p2}
% What is the difference between $N_n(t)$ and $N(t)$?
% \begin{solution}
%  $N_n(t)$ is a binomially distributed random variable with parameters~$n$ and~$p$.
%  The maximum value of $N_n(t)$ is~$n$.
%  The random variable $N(t)$ models the number of arrivals that can occur during $[0,t]$.
%  As such it is not necessarily bounded by~$n$.
%  Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable.
% \end{solution}
% \end{exercise}


\begin{exercise} \label{ex:p-355}
 Show \cref{eq:8} and  \cref{eq:10}
\begin{hint} Use the derivation of the case $\P{N(t)=0}$.
\end{hint}
\begin{solution}
 \begin{align*}
 \P{N(t+\Delta t) = n +1 \given N(t) = n}
&= \frac{\P{N(t+\Delta t) = n +1 , N(t) =n}}{P{N(t) = n}}\\
&= \P{N(t, t+\Delta t] = 1} = e^{-\lambda \Delta t} \frac{(\lambda \Delta t)^1}{1!} \\
&= (1-\lambda \Delta t + o(\Delta t))\lambda \Delta t = \lambda \Delta t - \lambda^2 \Delta t^2 + o(\Delta t) \\
&= \lambda \Delta t + o(\Delta t),\\
 \P{N(t+\Delta t) \geq n+2 \given N(t) = n}
&= \P{N(t, t+\Delta t] \geq 2} \\
&= 1 - \P{N(t, t+\Delta t] = 0} - \P{N(t, t+\Delta t] = 1}\\
& = 1- (1-\lambda \Delta t) - \lambda \Delta t +o(\Delta t)\\
&= o(\Delta t).
% &= e^{-\lambda \Delta t} \sum_{i=2}^\infty \frac{(\lambda \Delta t)^i}{i!}
% = e^{-\lambda \Delta t} \left(\sum_{i=0}^\infty \frac{(\lambda \Delta t)^i}{i!} - \lambda \Delta t - 1\right)\\
% &= e^{-\lambda \Delta t}(e^{\lambda \Delta t} - 1 - \lambda \Delta t)
% = 1 - e^{-\lambda \Delta t}(1 + \lambda \Delta t) \\
% &= 1 - (1-\lambda \Delta t + o(\Delta t))(1+\lambda \Delta t)
% = 1 - (1-\lambda^2 \Delta t^2 + o(\Delta t)) \\
% &= \lambda^2 \Delta t^2 + o(\Delta t) = o(\Delta t),
 \end{align*}
\end{solution}
\end{exercise}

% \begin{exercise} \label{ex:p-35}
%  Show
% \begin{hint}
% See the hint for~\cref{ex:p-355}, and  use $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x - 1$.
% \end{hint}
% \begin{solution}
%  \begin{align*}
%  \P{N(t+\Delta t) \geq n+2 \given N(t) = n}
% &= \P{N(t, t+\Delta t] \geq 2} \\
% &= e^{-\lambda \Delta t} \sum_{i=2}^\infty \frac{(\lambda \Delta t)^i}{i!}
% = e^{-\lambda \Delta t} \left(\sum_{i=0}^\infty \frac{(\lambda \Delta t)^i}{i!} - \lambda \Delta t - 1\right)\\
% &= e^{-\lambda \Delta t}(e^{\lambda \Delta t} - 1 - \lambda \Delta t)
% = 1 - e^{-\lambda \Delta t}(1 + \lambda \Delta t) \\
% &= 1 - (1-\lambda \Delta t + o(\Delta t))(1+\lambda \Delta t)
% = 1 - (1-\lambda^2 \Delta t^2 + o(\Delta t)) \\
% &= \lambda^2 \Delta t^2 + o(\Delta t) = o(\Delta t),
%  \end{align*}
% where we expand
% \begin{align*}
% (1-\lambda \Delta t + o(\Delta t))(1+\lambda \Delta t)
% &= 1 + \lambda \Delta t - \lambda \Delta t - \lambda^{2} \Delta^{2} t^{2} + o(\Delta t).
% \end{align*}
% We can also use the results of the previous parts to see that
% \begin{align*}
%  \P{N(t+\Delta t) \geq n+2 \given N(t) = n}
% &= \P{N(t, t+\Delta t] \geq 2} = 1- \P{N(t, t+\Delta t]<2} \\
% &= 1 - \P{N(t, t+\Delta t]= 0} - \P{N(t, t+\Delta t]=1} \\
% &= 1 - (1-\lambda \Delta t + o(\Delta t) ) - (\lambda \Delta t + o(\Delta t)) \\
% &= o(\Delta t).
% \end{align*}
% \end{solution}
% \end{exercise}



\begin{exercise}\label{ex:pois-mgf}
When $N \sim \Pois{\lambda}$ and $\alpha > 0$, show that $\E{\alpha^{N}} = e^{\lambda(\alpha-1)}$.
Use this to see that $M_{N(t)}(s) = \exp{(\lambda t(e^s-1))}$. Then  find $\E{N(t)}$ and $\V{N(t)}$.
\begin{hint}
LOTUS: $\E{\alpha^{N}} = \sum_{k=0}^\infty \alpha^{k} \P{N=k}$. Use~\cref{eq:69} and~\cref{eq:64}.
\end{hint}
\begin{solution}
Check the hint.
\begin{align*}
\sum_{k=0}^\infty \alpha^{k} \P{N=k} = \sum_{k=0}^\infty \alpha^{k} \frac{(\lambda)^k}{k!} e^{-\lambda}
= e^{-\lambda} \sum_{k=0}^\infty \frac{(\alpha \lambda)^k}{k!} =\exp(\lambda (\alpha - 1)).
\end{align*}

Take $\alpha=e^{s}$ in~\cref{ex:pois-mgf}.

 \begin{equation*}
 M_{N(t)}'(s) = \lambda t e^s \exp(\lambda t(e^s - 1)).
 \end{equation*}
 Hence $\E{N(t)} = M_{N(t)}'(0) = \lambda t $.
Next, $M_{N(t)}''(s) = (\lambda t e^s + (\lambda t e^s)^2) \exp(\lambda t(e^s - 1))$, hence $\E{(N(t))^2} = M''(0) = \lambda t + (\lambda t)^2$, and thus, $\V{N(t)} =\E{(N(t))^2}-(\E{N(t)})^2 = \lambda t + (\lambda t)^2 - (\lambda t)^2 = \lambda t$.

\end{solution}
\end{exercise}



\begin{exercise}\label{ex:30a}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show that
 \begin{equation*}
 \P{N_\lambda(t) = 1 \given N_\lambda(t) + N_\mu(t) = 1} =
\frac{\lambda}{\lambda+\mu}.
 \end{equation*}
In words,  given that a customer arrived in $[0,t]$, the probability that it is of the first type is $\lambda/(\lambda+\mu)$.

Use the exact same idea to derive \cref{eq:22}.
\begin{hint}
Use the standard formula for conditional probability and that $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$.
\end{hint}
\begin{solution}
 With the above:
 \begin{align*}
& \P{N_\lambda(t) = 1 \given N_\lambda(t) + N_\mu(t) = 1}
%= \frac{\P{N_\lambda(t) = 1, N_\lambda(t) + N_\mu(t) = 1}}{\P{N_\lambda(t) + N_\mu(t) = 1}} \\
%&= \frac{\P{N_\lambda(t) = 1, N_\mu(t) = 0}}{\P{N_{\lambda+\mu}(t) = 1}}
= \frac{\P{N_\lambda(t) = 1}\P{N_\mu(t) = 0}}{\P{N_{\lambda+\mu}(t) = 1}} \\
&= \frac{\lambda t \exp(-\lambda t) \exp(-\mu t)}{((\lambda+\mu)t)\exp{(-(\lambda+\mu)t)}}
= \frac{\lambda t \exp{(-(\lambda + \mu)t)}}{((\lambda+\mu)t)\exp{(-(\lambda+\mu)t)}}
= \frac{\lambda}{\lambda+\mu}.
 \end{align*}

\begin{align*}
 \P{N(0,s] =1\given N(0,t]=1}
% &= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}}
% = \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} = \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
\end{align*}
\end{solution}
\end{exercise}


\begin{exercise}\label{ex:41}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, use moment-generating functions to show that $N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$.
\begin{hint}
 Use~\cref{eq:73} and~\cref{eq:75}.
\end{hint}

\begin{solution}
\begin{align*}
M_{N_\lambda(t)+N_\mu(t)}(s)
&= M_{N_\lambda(t)}(s)\cdot M_{N_{\mu}(t)}(s)
=\exp(\lambda t (e^s -1))\cdot \exp(\mu t(e^s-1)) \\
&= \exp((\lambda + \mu)t (e^s-1)).
\end{align*}
\end{solution}
\end{exercise}



% \begin{exercise}\label{ex:p-36}
%  Show \cref{eq:22}.
% \begin{hint}
% %  Observe that
% %  \begin{equation*}
% % %\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
% % \1{N(0,s]+N(s,t]=1}\1{N(0,s]=1} = \1{1+N(s,t]=1}\1{N(0,s]=1}=\1{N(s,t]=0}\1{N(0,s]=1}.
% %  \end{equation*}
% Use independence and the Poisson distribution.
% \end{hint}
% \begin{solution}
% From the hint,
% \begin{align*}
%  \P{N(0,s] =1\given N(0,t]=1}
% % &= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}}
% % = \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
% &= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} = \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
% \end{align*}
% \end{solution}
% \end{exercise}



% \begin{exercise} \label{ex:53}
% \begin{solution}
% %Use~\cref{eq:66} with $f(x) = e^{sx}$.
% \end{solution}
% % \begin{solution}
% % \begin{align*}
% % M_{N(t)}(s)
% % &= \E{e^{s N(t)}}
% % = \sum_{k=0}^\infty e^{s k} \P{N(t)=k}
% % = \sum_{k=0}^\infty e^{s k} \frac{(\lambda t)^k}{k!} e^{-\lambda t} \\
% % &= e^{-\lambda t} \sum_{k=0}^\infty \frac{(e^s \lambda t)^k}{k!}
% % = \exp(-\lambda t + e^s \lambda t) =\exp(\lambda t(e^s - 1)).
% % \end{align*}
% % \end{solution}
% \end{exercise}


% \begin{exercise} \label{ex:l-101}
% \begin{hint}
% \end{hint}
% \begin{solution}
% \end{solution}
% \end{exercise}



% \begin{exercise} \label{ex:l-102}
%  Show that the SCV of $N(t)\sim P(\lambda t)$ is equal to $1/(\lambda t)$. What does this mean for~$t$ large?
% \begin{solution}
%  \begin{equation*}
% SCV = \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{\lambda t}.
%  \end{equation*}
% The relative variability of the Poisson process goes down as $t\to\infty$.
% \end{solution}
% \end{exercise}



\begin{exercise}\label{ex:1}
 Show with moment-generating functions that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability~$p$ results in a Poisson process $N_{\lambda p}$.
\begin{hint}
Dropping the dependence of~$N$ on~$t$ for the moment for notational convenience, consider the random variable
 \begin{equation*}
 Y = \sum_{i=1}^N Z_i,
 \end{equation*}
 with $N\sim P(\lambda)$ and $Z_i\sim B(p)$ and $\{Z_{i}\}$ iid.
Show that the moment-generating function of~$Y$ is equal to the moment-generating function of a Poisson random variable with parameter $\lambda p$.
\end{hint}
\begin{solution}
% Consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that~$n$
% arrivals occurred. Then we throw~$n$ independent coins with success probability
% $p$. It is clear that~$Y$ is indeed a thinned Poisson random variable.

% Model the coins as a generic Bernoulli distributed random variable~$Z$.
Use the hint. Then,
\begin{equation*}
 \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p,
\end{equation*}
from which
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n,
\end{equation*}
where we use~\cref{eq:73} and that the $Z_i$  are iid.
Thus, $\E{e^{sY}\big|N=n} = \E{e^{s\sum_{i=1}^n Z_i}} = \alpha^{n}$, where we write $\alpha =1 + p (e^s - 1)$ for ease.
Now with Adam's law and~\cref{ex:pois-mgf}, we get
\begin{equation*}
\E{e^{sY}} = \E{\alpha^{N}} = e^{\lambda (\alpha - 1)} = \exp(\lambda p (e^s - 1)).
\end{equation*}
This is the MGF of a Poisson rv with rate $\lambda p$.

% With~\cref{eq:77},
% \begin{align*}
%  \E{e^{s Y}}
% &= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}}
% = \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}}
% = \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
% &= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}},\quad\text{by independence of $Z_i$ and~$N$}, \\
% &= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
% &= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
% = e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
% &= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
% \end{align*}
% \begin{align*}
%  \E{e^{s Y}}
% &= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}}
% = \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}}
% = \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
% &= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}},\quad\text{by independence of $Z_i$ and~$N$}, \\
% &= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
% &= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
% = e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
% &= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
% \end{align*}
\end{solution}
\end{exercise}


\begin{exercise} \label{ex:96}
 Use moment-generating functions to prove~\cref{eq:bin}.
\begin{hint}
Solve \cref{ex:1} first.
\end{hint}
\begin{solution}
Take $Y=\sum_{i=1}^n Z_i$ with $Z_i\sim B(p)$. Then,
\begin{equation*}
M_Y(s) = \E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n.
\end{equation*}
Recall that $p= \lambda t/ n$. Then, with~\cref{eq:76},
\begin{equation*}
\lim_{n\to\infty} \left(1 + \frac{\lambda t}{n} (e^s - 1)\right)^n = \exp({\lambda t (e^s-1)}).
\end{equation*}
\end{solution}
\end{exercise}


% \noindent
% You can skip the rest of these exercises if you are happy with the above proofs that are based on MGFs.
% If you feel more comfortable with conditioning and summations, then here are some more straightforward proofs.

% \begin{exercise} \label{ex:31}
%  Show~\cref{eq:bin}
% \begin{hint}
%  First find~$p$, $n$, $\lambda$ and~$t$ such that the rate at which an event occurs in both processes are the same.
%  Then consider the binomial distribution and use the standard limit $(1-x/n)^n \to e^{-x}$ as $n\to \infty$.
% \end{hint}
% \begin{solution}
% Write $N=\lambda t n$, so that $p=1/n = \lambda t /N$. Then
%  \begin{align*}
%  {N \choose k} \left(\frac{\lambda t}{N}\right)^k \left(1-\frac{\lambda t}N \right)^{N-k}
% &= \frac{N!}{k!(N-k)!} \left(\frac{\lambda t}{N}\frac{N}{N-\lambda t}\right)^k \left(1-\frac{\lambda t}N\right)^{N} \\
% &= \frac{(\lambda t)^k}{k!} \left(\frac N{N-\lambda t} \right)^k \frac{N!}{N^k(N-k)!}\left(1-\frac{\lambda t}N\right)^{N}\\
% &= \frac{(\lambda t)^k}{k!} \left(\frac N{N-\lambda t} \right)^k \frac{N}{N}\frac{N-1}{N}\cdots\frac{N-k+1}{N} \left(1-\frac{\lambda t}N\right)^{N}.
% \end{align*}
% Observe now that, as $\lambda t$ is finite, $N/(N-\lambda t)\to 1$ as
% $n$, hence~$N$, $\to \infty$. Also for any finite~$k$, $(N-k)/N\to1$. Finally, use~\cref{eq:76} to see that
% $\left(1-\frac{\lambda t}N\right)^{N} \to e^{-\lambda t}$.
% \end{solution}
% \end{exercise}

% \begin{exercise} \label{ex:2}
% Show that $\E{N(t)}  = \lambda t$.
% \begin{hint}
% Use~\cref{eq:66}. Note that the term with $n=0$ does not contribute in the following summation
% \begin{equation*}
% \sum_{n=0}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty \frac{\lambda^n}{(n-1)!} = \lambda \sum_{n=0}^\infty \frac{\lambda^n}{n!} = \lambda e^{\lambda},
% \end{equation*}
% where we apply a change of notation in the second to last step.
% \end{hint}
% \begin{solution}
%  When a random variable~$N$ is Poisson distributed with parameter
%  $\lambda t$,
%  \begin{align*}
%  \E N
% &= \sum_{n=0}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}
% = \sum_{n=1}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!}
% = e^{-\lambda t} \lambda t \sum_{n=1}^\infty \frac{(\lambda t)^{n-1}}{(n-1)!} \\
% &= e^{-\lambda t} \lambda t \sum_{n=0}^\infty \frac{(\lambda t)^{n}}{n!}
% = e^{-\lambda t} \lambda t e^{\lambda t}
% = \lambda t.
%  \end{align*}
% \end{solution}
% \end{exercise}


% \begin{exercise}\label{ex:p-1}
% Show that $\E{(N(t))^{2}}  = (\lambda t)^{2} + \lambda t$
% \begin{solution}
%  \begin{align*}
%  \E{N^2}
% &= \sum_{n=0}^\infty n^2 e^{-\lambda t}\frac{(\lambda t)^n}{n!}
% = e^{-\lambda t} \sum_{n=1}^\infty n \frac{(\lambda t)^n}{(n-1)!}
% = e^{-\lambda t} \sum_{n=0}^\infty (n+1) \frac{(\lambda t)^{n+1}}{n!} \\
% &= e^{-\lambda t} \lambda t \sum_{n=0}^\infty n \frac{(\lambda t)^{n}}{n!} +e^{-\lambda t}\lambda t \sum_{n=0}^\infty\frac{(\lambda t)^{n}}{n!}
% = (\lambda t)^2 + \lambda t.
% \end{align*}
% \end{solution}
% \end{exercise}

% \begin{exercise}\label{ex:pe-53}
%  Show that $\V{N(t)}  = \lambda t.$
% \begin{hint} Use~\cref{eq:68}, \cref{ex:p-1},  and~\cref{ex:2}.
% \end{hint}
% \begin{solution}
% $\V N = \E{N^2} - (\E N)^2 = (\lambda t)^2 + \lambda t - (\lambda t)^2 = \lambda t$.
% \end{solution}
% \end{exercise}



% \begin{exercise}\label{ex:l-103}
% If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show with a conditioning argument that
% the merged process $N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$.
% \begin{hint}
%  Use sets $\{N_\lambda(t) = i\}$ to decompose $\{N_\lambda(t) + N_\mu(t) = n\}$. With this observe that
%  \begin{equation*}
%  \1{N_\lambda(t) + N_\mu(t) = n} =
%  \sum_{i=0}^n \1{N_\lambda(t)=i, N_\mu(t) = n-i}.
%  \end{equation*}
% Take expectations left and right, use~\cref{eq:74}, and independence of $N_\lambda$ and $N_\mu$. Near the end of the computation, use~\cref{eq:71}.
% \end{hint}
% \begin{solution}
% \begin{align*}
% \P{N_\lambda(t) + N_\mu(t) = n}
% &= \sum_{i=0}^n \P{N_\mu(t) = n-i}\P{N_\lambda(t)=i} \\
% &= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t}
% = e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} \\
% &= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i \quad\text{(binomial formula)} \\
% &= \frac{((\mu+\lambda)t)^n}{n!}e^{-(\mu+\lambda)t}.
%  \end{align*}
% \end{solution}
% \end{exercise}


% \begin{exercise}\label{ex:32}
%  Show with conditioning that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability~$p$ results in a Poisson process $N_{\lambda p}$.
% \begin{hint}
% Suppose that $N_1$ is the thinned stream, and~$N$ the original stream. Condition on the total number of arrivals $N(t)=n$ up to time
%  $t$. Then, realize that the probability that a person is of type 1 is~$p$. Hence, when you consider~$n$ people in
%  total, the number $N_1(t)$ of type 1 people is binomially distributed. Thus, given that~$n$ people arrived, the probability of~$k$ `successes' (i.e., arrivals of type 1), is
%  \begin{equation*}
%  \P{N_1(t)=k \given N(t) = n} = {n \choose k} p^k (1-p)^{n-k}.
%  \end{equation*}
% Use~\cref{eq:70} to decompose the $\{N_1=k\}$, and~\cref{eq:76} at the end.
% \end{hint}
% \begin{solution}
% \begin{align*}
%  \P{N_1(t) = k}
% &= \sum_{n=k}^\infty \P{N_1(t) =k, N(t) = n}
% = \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}\P{N(t)=n} \\
% &= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
% &= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda t} \frac{(\lambda t)^n}{n!}, \quad\text{by the hint}\\
% &= e^{-\lambda t}\sum_{n=k}^\infty \frac{p^k (1-p)^{n-k}}{k! (n-k)!} (\lambda t)^n
% = e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=k}^\infty \frac{(\lambda t (1-p))^{n-k}}{(n-k)!}\\
% &= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=0}^\infty \frac{(\lambda t (1-p))^{n}}{n!}
% = e^{-\lambda t} \frac{(\lambda t p)^k}{k!} e^{\lambda t(1-p)} = e^{-\lambda t p} \frac{(\lambda t p)^k}{k!}.
% \end{align*}
% \end{solution}
% \end{exercise}

\input{trailer}
